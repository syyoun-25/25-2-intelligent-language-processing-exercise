{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syyoun-25/25-2-intelligent-language-processing-exercise/blob/main/word_embeddings_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title-md"
      },
      "source": [
        "# Word2Vec (CBOW vs. Skip-gram), GloVe, and 2D Visualization\n",
        "\n",
        "1) 공개 데이터셋 **text8**로 **Word2Vec** 모델을 CBOW/Skip-gram 방식으로 각각 학습\n",
        "2) **사전 학습된 GloVe (glove-wiki-gigaword-100)** 모델로 단어 관계 확인 실습\n",
        "3) **t-SNE**로 선택한 단어 벡터를 2D로 시각화\n",
        "\n",
        "**Colab 사용**: 런타임 환경 연결 -- 구글 드라이브 연결 -- 의존성 설치\n"
      ],
      "id": "title-md"
    },
    {
      "cell_type": "code",
      "source": [
        "# 구글 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yvRwP4wyaOtZ"
      },
      "id": "yvRwP4wyaOtZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 의존성 설치 (Colab 최초 실행 시)\n",
        "!pip install gensim\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "o9yxuBC-cOOw",
        "outputId": "533eec44-85c7-492c-954f-5b8304fd3869",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "o9yxuBC-cOOw",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "execution_count": 2,
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE"
      ],
      "id": "imports"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sec1-md"
      },
      "source": [
        "## Section 1: Word2Vec 모델 학습 (CBOW vs. Skip-gram)"
      ],
      "id": "sec1-md"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sec1-code"
      },
      "outputs": [],
      "execution_count": null,
      "source": [
        "# ================================================================= #\n",
        "# Section 1: Word2Vec 모델 학습 (CBOW vs. Skip-gram)\n",
        "# ================================================================= #\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"1. Word2Vec 모델 학습 (CBOW vs. Skip-gram)\")\n",
        "print(\"공개 데이터셋으로 두 학습 방식의 차이를 비교해 봅시다.\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 공개 데이터셋 'text8' 다운로드\n",
        "# 'text8': 약 170만 개 단어 토큰으로 구성된 단일 텍스트 파일 -- Wikipedia 글 일부를 선형적으로 연결한 텍스트\n",
        "print(\"\\n공개 데이터셋 'text8'을 다운로드합니다...\")\n",
        "try:\n",
        "    corpus = list(api.load('text8'))\n",
        "    print(\"다운로드 완료!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n데이터셋 다운로드 중 오류가 발생했습니다: {e}\")\n",
        "    print(\"인터넷 연결을 확인하거나 gensim-data 설치를 다시 시도해주세요.\")\n",
        "    raise SystemExit\n",
        "\n",
        "# 데이터셋의 파이썬 객체 타입 확인\n",
        "print(\"\\n[데이터셋의 타입]:\", type(corpus))\n",
        "\n",
        "# 데이터셋의 첫 번째 리스트 확인 (각 리스트는 10,000개의 단어로 구성되어 있다.)\n",
        "print(\"\\n[데이터셋의 첫 번째 문장(단어 리스트) 확인]:\")\n",
        "print(corpus[0])\n",
        "\n",
        "# 데이터셋의 첫 번째 문장의 타입 확인\n",
        "print(\"\\n[첫 번째 문장의 타입]:\", type(corpus[0]))\n",
        "\n",
        "# 데이터셋의 첫 번째 문장에 포함된 단어의 수 확인\n",
        "print(\"\\n[첫 번째 문장에 포함된 단어의 수]:\", len(corpus[0]))\n",
        "\n",
        "# 데이터셋의 총 리스트 수와 총 단어 수 확인\n",
        "total_sentences = len(corpus)\n",
        "total_words = sum(len(sentence) for sentence in corpus)\n",
        "print(\"\\n[데이터셋 통계]\")\n",
        "print(f\"총 문장(리스트) 수: {total_sentences}\")\n",
        "print(f\"총 단어 수: {total_words}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# CBOW (Continuous Bag of Words) 모델 학습\n",
        "# sg=0: 주변 단어들을 이용해 중간 단어를 예측 (빠르고 효율적)\n",
        "print(\"\\nCBOW 모델 학습...\")\n",
        "cbow_model = Word2Vec(\n",
        "    sentences=corpus,\n",
        "    vector_size=100, # 워드 벡터의 특징 값, 즉 임베딩된 벡터의 차원\n",
        "    window=5,        # 컨텍스트 윈도우 크기\n",
        "    min_count=100,   # 단어 최소 빈도 수 제한 (최소 출현 빈도 100회 미만 단어는 무시)\n",
        "    sg=0             # 0은 CBOW\n",
        ")\n",
        "\n",
        "# skip-gram 모델 학습\n",
        "# sg=1: 중간 단어를 이용해 주변 단어들을 예측 (성능이 더 좋지만 학습 속도가 상대적으로 느림)\n",
        "print(\"\\nSkip-gram 모델 학습...\")\n",
        "skipgram_model = Word2Vec(\n",
        "    sentences=corpus,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=100,\n",
        "    sg=1              # 1은 skip-gram\n",
        ")\n",
        "\n",
        "print(\"\\n[CBOW 모델 결과 - 'king'과 가장 유사한 단어 3개]\")\n",
        "print(cbow_model.wv.most_similar('king', topn=3))\n",
        "\n",
        "print(\"\\n[Skip-gram 모델 결과 - 'king'과 가장 유사한 단어 3개]\")\n",
        "print(skipgram_model.wv.most_similar('king', topn=3))\n",
        "\n",
        "# 모델 저장하기\n",
        "cbow_model.save(\"cbow_model.model\")\n",
        "skipgram_model.save(\"skipgram_model.model\")"
      ],
      "id": "sec1-code"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sec2-md"
      },
      "source": [
        "## Section 2: 사전 학습된 GloVe 모델 활용"
      ],
      "id": "sec2-md"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sec2-code"
      },
      "outputs": [],
      "execution_count": null,
      "source": [
        "# =================================================================\n",
        "# Section 2: 사전 학습된 GloVe 모델 활용\n",
        "# =================================================================\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*60)\n",
        "print(\"2. 사전 학습된 GloVe 모델 활용\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# gensim에서 제공하는 'glove-wiki-gigaword-100' (GloVe 모델)을 다운로드\n",
        "print(\"\\n사전 학습된 'glove-wiki-gigaword-100' 모델 다운로드...\")\n",
        "try:\n",
        "    glove_model = api.load('glove-wiki-gigaword-100')\n",
        "    print(\"다운로드 완료!\")\n",
        "\n",
        "    # 단어 관계 추론\n",
        "    # 'king' - 'man' + 'woman' = ? -> 'queen'\n",
        "    # 'seoul' - 'korea' + 'japan' = ? -> 'tokyo'\n",
        "    print(\"\\n[단어 관계 추론 실습]\")\n",
        "    result = glove_model.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\n",
        "    print(f\"king - man + woman = {result[0][0]}\")\n",
        "\n",
        "    result_city = glove_model.most_similar(positive=['seoul', 'japan'], negative=['korea'], topn=1)\n",
        "    print(f\"seoul - korea + japan = {result_city[0][0]}\")\n",
        "\n",
        "    result_city2 = glove_model.most_similar(positive=['france', 'tokyo'], negative=['paris'], topn=1)\n",
        "    print(f\"france - paris + tokyo = {result_city2[0][0]}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n모델 다운로드 또는 사용 중 오류가 발생했습니다: {e}\")\n",
        "    print(\"인터넷 연결을 확인하거나 라이브러리 설치를 다시 시도해주세요.\")\n"
      ],
      "id": "sec2-code"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sec3-md"
      },
      "source": [
        "## Section 3: 워드 벡터 시각화 (t-SNE)"
      ],
      "id": "sec3-md"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sec3-code"
      },
      "outputs": [],
      "execution_count": null,
      "source": [
        "# =================================================================\n",
        "# Section 3: 워드 벡터 시각화\n",
        "# =================================================================\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*60)\n",
        "print(\"3. 워드 벡터 시각화\")\n",
        "print(\"추상적인 벡터를 2D 공간에 시각화하여 단어의 관계를 이해합니다.\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 모델 로드하기\n",
        "loaded_cbow_model = Word2Vec.load(\"cbow_model.model\")\n",
        "loaded_skipgram_model = Word2Vec.load(\"skipgram_model.model\")\n",
        "\n",
        "# 시각화할 단어 리스트를 정의합니다.\n",
        "words_to_visualize = ['king', 'queen', 'man', 'woman', 'berlin', 'germany', 'paris', 'france', 'apple', 'orange', 'computer', 'science']\n",
        "\n",
        "# (안전장치) 모델의 vocabulary에 존재하는 단어만 사용\n",
        "present_words_cbow = [w for w in words_to_visualize if w in loaded_cbow_model.wv.key_to_index]\n",
        "present_words_skip = [w for w in words_to_visualize if w in loaded_skipgram_model.wv.key_to_index]\n",
        "if len(present_words_cbow) < len(words_to_visualize) or len(present_words_skip) < len(words_to_visualize):\n",
        "    print(\"일부 단어가 vocabulary에 없어 제외되었습니다.\")\n",
        "    print(\"CBOW 사용 단어:\", present_words_cbow)\n",
        "    print(\"Skip-gram 사용 단어:\", present_words_skip)\n",
        "\n",
        "vectors_cbow = [loaded_cbow_model.wv[word] for word in present_words_cbow]\n",
        "vectors_skipgram = [loaded_skipgram_model.wv[word] for word in present_words_skip]\n",
        "\n",
        "# TSNE로 100차원 벡터를 2차원으로 축소\n",
        "tsne_cbow = TSNE(n_components=2, perplexity=5, random_state=42)\n",
        "reduced_vectors_cbow = tsne_cbow.fit_transform(np.array(vectors_cbow))\n",
        "tsne_skip = TSNE(n_components=2, perplexity=5, random_state=42)\n",
        "reduced_vectors_skipgram = tsne_skip.fit_transform(np.array(vectors_skipgram))\n",
        "\n",
        "path = '/content/drive/MyDrive/25-2-intelligent-language-processing/' # 시각화 결과를 저장할 경로를 입력하세요.\n",
        "\n",
        "# 시각화\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.scatter(reduced_vectors_cbow[:, 0], reduced_vectors_cbow[:, 1], edgecolors='black', c='cornflowerblue', s=100, label='CBOW')\n",
        "for i, word in enumerate(present_words_cbow):\n",
        "    plt.annotate(word, xy=(reduced_vectors_cbow[i, 0], reduced_vectors_cbow[i, 1]), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
        "\n",
        "plt.scatter(reduced_vectors_skipgram[:, 0], reduced_vectors_skipgram[:, 1], edgecolors='black', c='lemonchiffon', s=100, label='Skip-gram')\n",
        "for i, word in enumerate(present_words_skip):\n",
        "    plt.annotate(word, xy=(reduced_vectors_skipgram[i, 0], reduced_vectors_skipgram[i, 1]), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
        "\n",
        "plt.title('Word Embedding Visualization (CBOW vs. Skip-gram) - text8')\n",
        "plt.xlabel('TSNE Component 1')\n",
        "plt.ylabel('TSNE Component 2')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.savefig(f'{path}word_embedding_visualization_text8.png', dpi=150, bbox_inches='tight')\n",
        "print(f\"\\n시각화 결과가 '{path}word_embedding_visualization_text8.png' 파일로 저장되었습니다.\")\n"
      ],
      "id": "sec3-code"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h_DilVSDf_zh"
      },
      "id": "h_DilVSDf_zh",
      "execution_count": null,
      "outputs": []
    }
  ]
}